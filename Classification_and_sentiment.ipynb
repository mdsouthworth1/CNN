{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a403ff-f1db-41b2-b42d-330172a64a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors, word2vec\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from scipy.spatial.distance import cosine as cosDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89d32a8-8259-4345-b7ba-9236acfaf41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d25dc-5874-4408-a014-ed6bb6590e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel(Model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf39991-4bd0-4e3c-a357-6c677f55aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global w2vModel\n",
    "\n",
    "w2vModel = word2vec.KeyedVectors.load_word2vec_format(\"word2vec-google-news-300.gz\",binary=True)\n",
    "\n",
    "    #path = get_tmpfile('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89eddd9c-912b-45b9-bb88-fc71b3c8bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, target =\t(\n",
    "  [\n",
    "    \"The earthquake caused severe damage to 135 houses in Mirpur District, with a further 319 being partially damaged, most in Mirpur and just four in Bhimber District.\",\n",
    "    \" Two bridges were reported damaged and parts of several roads were affected, particularly 14 km of the Main Jatlan Road.\",\n",
    "    \" According to the chairman of Pakistan's National Disaster Management Authority (NDMA), 'In Mirpur, besides the city, a small town Jatlan, and two small villages Manda and Afzalpur' were among the worst-hit areas.\",\n",
    "    \" According to him, the main road which runs alongside a river from Mangla to Jatla suffered major damage.\",\n",
    "    \" According to the officials, the Mangla Dam, Pakistan's major water reservoir, was spared.\",\n",
    "    \" However, the dam's power house was closed, which resulted in a loss of 900 megawatts to Pakistan's national power grid.\",\n",
    "    \" At 7:20 pm, power generation at Mangla was resumed, restoring 700 MW to the national grid.\",\n",
    "  ],\n",
    "  \"As per the administrator of Pakistan's National Disaster Management Authority (NDMA), 'In Mirpur, other than the city, a community Jatlan, and two little towns Manda and Afzalpur' were among the most exceedingly awful hit regions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623468e4-cc02-41f9-a79e-5ce8a1417bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPlagiarism(sentences, target):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    target = re.sub(r'[^a-z0-9\\s]', '', target.lower())\n",
    "    target = target.split()\n",
    "    print(target)\n",
    "    target_words = []\n",
    "    target_words = [lemmatizer.lemmatize(word) for word in target if word not in stop_words]\n",
    "    #words = set(words)\n",
    "\n",
    "    target_vectors = [w2vModel[word] for word in target_words if word in w2vModel]\n",
    "    target_vector = np.mean(target_vectors, axis = 0)\n",
    "\n",
    "    #print(target_vector)\n",
    "\n",
    "    words = []\n",
    "    cosine_distance = []\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence.lower())\n",
    "        sentence = sentence.split()\n",
    "        words = [lemmatizer.lemmatize(word) for word in sentence if word not in stop_words]\n",
    "        #print(words)\n",
    "        vectors = [w2vModel[word] for word in words if word in w2vModel]\n",
    "        avg_vector = np.mean(vectors, axis = 0)\n",
    "        #print(avg_vector)\n",
    "        cosine = cosDist(target_vector, avg_vector)\n",
    "        #print(cosine)\n",
    "        cosine_distance.append(cosine)\n",
    "\n",
    "    target_index = np.argmin(cosine_distance)\n",
    "\n",
    "    #print(target_index)\n",
    "    return target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0c5243-5de1-4624-b0e9-2b577688af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'per', 'the', 'administrator', 'of', 'pakistans', 'national', 'disaster', 'management', 'authority', 'ndma', 'in', 'mirpur', 'other', 'than', 'the', 'city', 'a', 'community', 'jatlan', 'and', 'two', 'little', 'towns', 'manda', 'and', 'afzalpur', 'were', 'among', 'the', 'most', 'exceedingly', 'awful', 'hit', 'regions']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPlagiarism(sentences, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaf9df-6668-45dd-838a-fd139b9f01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySubreddit_train(file):\n",
    "\n",
    "    global lr_w2v\n",
    "    global Scaler\n",
    "    global le\n",
    "    lr = LogisticRegression(penalty = 'l2', solver = 'sag', max_iter = 2000)\n",
    "\n",
    "    #file = 'redditComments_train.jsonlist'\n",
    "    with open(file,  encoding='utf-8') as json_file:\n",
    "        training_set = [json.loads(line) for line in json_file]\n",
    "\n",
    "    train_class = [line['subreddit'] for line in training_set]\n",
    "    train_comments = [line['body'] for line in training_set]\n",
    "\n",
    "    #pd.Series(subreddit_class).value_counts()\n",
    "\n",
    "    x_train = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    url_pattern = r'\\((https?://[^\\s]+)\\)'\n",
    "    replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "    #replacements = lambda text: ''.join(replacement_rules.get(char, char) for char in text)\n",
    "    for comment in train_comments:\n",
    "        for char, replacement in replacement_rules.items():\n",
    "            comment = comment.replace(char, replacement)\n",
    "        #print('\\nnew comment line:', comment)\n",
    "        #for word in words:\n",
    "        #if re.match(r'http[s]?://', comment):\n",
    "        #Extract urls and add to token list. Text has useful urls for training\n",
    "        combined_tokens = []\n",
    "        urls = re.findall(url_pattern, comment)\n",
    "        for url in urls:\n",
    "            comment = re.sub(r'https?:\\/\\/(www\\.)?', ' ', comment)\n",
    "            sub_domain = url.split('/')\n",
    "            domain_name = sub_domain[0]\n",
    "            path_words = sub_domain[1:] if len(sub_domain) > 1 else []\n",
    "            #Split domain and rest of url. Domain will likely be class specific\n",
    "            split_domain = domain_name.split('.')\n",
    "            path_tokens = []\n",
    "            for i in path_words:\n",
    "                path_tokens.extend(re.split(r'[\\/\\-_+\\)\\(\\.]', i))            \n",
    "            split_url = split_domain + [token for token in path_tokens if token.strip()]\n",
    "            #Take care of edge cases\n",
    "            for j in split_url:\n",
    "                if ' ' in j:\n",
    "                    combined_tokens.extend(j.lower().split())\n",
    "                else:\n",
    "                    combined_tokens.append(j.lower())\n",
    "            for word in combined_tokens:\n",
    "                word = re.sub(r'\\W', '', word.lower())\n",
    "        #Remove processed urls or else duplicates\n",
    "        comment = re.sub(url_pattern, '', comment)\n",
    "        words = re.findall(r'\\b\\w+(?:\\'\\w+)?\\b', comment)\n",
    "        cleaned_words = []\n",
    "        for word in words:\n",
    "            if word not in combined_tokens:\n",
    "                if ' ' in word:\n",
    "                    cleaned_words.extend(word.lower().split())\n",
    "                else:\n",
    "                    cleaned_words.append(word.lower())\n",
    "        #print('CLEANED WORDS:', cleaned_words)\n",
    "        combined_tokens.extend(cleaned_words)\n",
    "        #combined_tokens = set(combined_tokens)\n",
    "        #print('Combined tokens:', combined_tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in combined_tokens if token not in stop_words]\n",
    "        #print('all tokens:', lemmatized_tokens)\n",
    "        comment_vectors = [w2vModel[token] for token in lemmatized_tokens if token in w2vModel]\n",
    "        x_train.append(np.mean(comment_vectors, axis = 0) if comment_vectors else np.zeros(w2vModel.vector_size))\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    y = np.array(train_class)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y)\n",
    "\n",
    "    Scaler = MinMaxScaler()\n",
    "    x_train_scaled = Scaler.fit_transform(x_train)\n",
    "\n",
    "    lr_w2v = lr.fit(x_train_scaled, y_train)\n",
    "    return lr_w2v, x_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db21ee0e-4d05-440d-8176-5ec77a17a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'redditComments_train.jsonlist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0382c9-ef82-4540-acaa-e7605aceab46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(max_iter=2000),\n",
       " array([[0.54297067, 0.48082819, 0.47993328, ..., 0.50062613, 0.52656252,\n",
       "         0.39240815],\n",
       "        [0.5234093 , 0.4801517 , 0.45260427, ..., 0.47301351, 0.49185644,\n",
       "         0.45293992],\n",
       "        [0.55963307, 0.38940024, 0.43514474, ..., 0.55232955, 0.50502742,\n",
       "         0.3720898 ],\n",
       "        ...,\n",
       "        [0.54166667, 0.41122328, 0.39849141, ..., 0.48622238, 0.55550874,\n",
       "         0.45559251],\n",
       "        [0.59729042, 0.49879816, 0.44997485, ..., 0.43743399, 0.54507289,\n",
       "         0.37392396],\n",
       "        [0.56649368, 0.45237159, 0.44338388, ..., 0.46795909, 0.52390996,\n",
       "         0.48116131]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifySubreddit_train(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a253643a-9e1f-4108-9fa0-f52ddea60443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifySubreddit_test(text):\n",
    "    with open(text,  encoding='utf-8') as json_file:\n",
    "        test_set = [json.loads(line) for line in json_file]\n",
    "        x_test = []\n",
    "\n",
    "        subreddit_class_test = [line['subreddit'] for line in test_set ]\n",
    "        comments_test = [line['body'] for line in test_set]\n",
    "    for comment in comments_test:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        url_pattern = r'\\((https?://[^\\s]+)\\)'\n",
    "        replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "        #replacements = lambda text: ''.join(replacement_rules.get(char, char) for char in text)\n",
    "        #comments = list(map(replacements, comments))\n",
    "        #print('\\nnew comment line:', comment)\n",
    "        for char, replacement in replacement_rules.items():\n",
    "            comment = comment.replace(char, replacement)\n",
    "        #for word in words:\n",
    "        #if re.match(r'http[s]?://', comment):\n",
    "        #Extract urls and add to token list. Text has useful urls for training\n",
    "        combined_tokens = []\n",
    "        urls = re.findall(url_pattern, comment)\n",
    "        for url in urls:\n",
    "            comment = re.sub(r'https?:\\/\\/(www\\.)?', ' ', comment)\n",
    "            sub_domain = url.split('/')\n",
    "            domain_name = sub_domain[0]\n",
    "            path_words = sub_domain[1:] if len(sub_domain) > 1 else []\n",
    "            #Split domain and rest of url. Domain will likely be class specific\n",
    "            split_domain = domain_name.split('.')\n",
    "            path_tokens = []\n",
    "            for i in path_words:\n",
    "                path_tokens.extend(re.split(r'[\\/\\-_+\\)\\(\\.]', i))            \n",
    "            split_url = split_domain + [token for token in path_tokens if token.strip()]\n",
    "            #Take care of edge cases\n",
    "            for j in split_url:\n",
    "                if ' ' in j:\n",
    "                    combined_tokens.extend(j.lower().split())\n",
    "                else:\n",
    "                    combined_tokens.append(j.lower())\n",
    "            #Remove stray punctuation\n",
    "            for word in combined_tokens:\n",
    "                word = re.sub(r'\\W', '', word.lower())\n",
    "        #Remove processed urls or else duplicates\n",
    "        comment = re.sub(url_pattern, '', comment)\n",
    "        words = re.findall(r'\\b\\w+(?:\\'\\w+)?\\b', comment)\n",
    "        cleaned_words = []\n",
    "        for word in words:\n",
    "            if word not in combined_tokens:\n",
    "                if ' ' in word:\n",
    "                    cleaned_words.extend(word.lower().split())\n",
    "                else:\n",
    "                    cleaned_words.append(word.lower())\n",
    "        combined_tokens.extend(cleaned_words)\n",
    "        #combined_tokens = set(combined_tokens)\n",
    "        #print('Combined tokens:', combined_tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in combined_tokens if token not in stop_words]\n",
    "        #print('all tokens:', lemmatized_tokens)\n",
    "        comment_vectors = [w2vModel[token] for token in lemmatized_tokens if token in w2vModel]\n",
    "        x_test.append(np.mean(comment_vectors, axis = 0) if comment_vectors else np.zeros(w2vModel.vector_size))\n",
    "    \n",
    "        x_test = np.array(x_test)\n",
    "        x_test_scaled = Scaler.transform(x_test)\n",
    "    \n",
    "        prediction = lr_w2v.predict(x_test_scaled)\n",
    "        #lr_w2v.predict_proba(x_test)\n",
    "    \n",
    "        predicted_class = le.inverse_transform(prediction)\n",
    "        print(predicted_class[0])\n",
    "        return predicted_class[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8fd559e-d02c-4a97-a6c3-d237693d89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"redditComments_test_notGraded.jsonlist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e119e74-6b56-48dd-8333-272a42b7799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newToTheNavy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'newToTheNavy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifySubreddit_test(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bfeb3-8c6f-444f-96f0-aa929be6f4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
